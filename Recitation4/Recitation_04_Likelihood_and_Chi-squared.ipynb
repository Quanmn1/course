{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "<i>This code was authored by the 8.S50x Course Team, Copyright 2021 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "# RECITATION 4: Likelihoods and the $\\chi^2$ Function\n",
    "\n",
    "<br>\n",
    "<!--end-block--> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.0 Overview of Learning Objectives\n",
    "\n",
    "In this recitation we will explore the following objectives:\n",
    "\n",
    "- Understand and construct likelihood and log-likelihood functions\n",
    "- Examine the $\\chi^2$ metric\n",
    "- Perform a fit by maximizing a log-likelihood function using `lmfit`\n",
    "\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.1 Warm-up Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x_i$ be the x coordinates of the data points we observe, and let $f(x_i)$ be the model we are trying to fit. We assume that the data that we observe is generated from this model with some extra error, usually do to some sort of noise.\n",
    "\n",
    "$$y_i = f(x_i) + \\epsilon $$\n",
    "\n",
    "where $\\epsilon$ is a random variable drawn from a Gaussian distribution with mean $0$. Usually, in most physics experiments, we will be able to quantify the width of the distribution by making repeated measurements and calculating the standard error, which will give us a vector of standard errors, $\\sigma_i$, which are usually plotted as errorbars like in the plot above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:red\">>>>QUESTION 4.1</span>\n",
    "\n",
    "a) So, we assume that $y_i$ is a random variable with mean $f(x_i)$, and variance $\\sigma_i^2$. Given this, what is the probability distribution for $y_i$?\n",
    "\n",
    "b) Given the answer to (a), and assuming that the probability distributions for each of the $y_i$ are independent, what is the joint probability distribution?\n",
    "\n",
    "\n",
    "<!--\n",
    "#solution\n",
    "The probability distribution is given by \n",
    "\n",
    "$$P(y_i) = \\frac{1}{\\sigma_i\\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2} \\frac{(y_i - f(x_i))^2}{\\sigma_i^2}\\right)$$\n",
    "\n",
    "If we assume each of the $y_i$ are indpendent random variables, we have the joint probabilty distribution given by\n",
    "\n",
    "$$P(y_1, y_2, ... ) = \\frac{1}{(2\\pi)^{k/2}\\prod_i\\sigma_i} \\exp\\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i))^2}{\\sigma_i^2}\\right)$$\n",
    "-->\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.2 The Likelihood Function\n",
    "\n",
    "The previous exercise gives the probability distribution of observing the data $y$ given a particular model $f(x)$ as a function of the independent data points $x$ and its parameters $\\alpha_1, \\alpha_2, \\alpha_3, ..., \\alpha_m $. This is known as the <b>likelihood</b> function $P(y|\\alpha)$. Note that the probability of observing the data given a particular model is *not* the same as the probability of a particular model being true given observed data (although they are related). To begin, however, we will assume that if we find the model that maximizes the probability of the observed data, this model is close to the true model. This technique is called <b>Maximum Likelihood Estimation</b> (MLE).\n",
    "\n",
    "Suppose we take the errors $\\epsilon$ in $y_i = f(x_i; \\alpha) + \\epsilon $ to be sampled from some general distribution $\\rho(y_i - f(x_i, \\alpha))$ instead of a Gaussian. We can then construct a  likelihood $P(y|\\alpha)$ (and corresponding log-likelihood) function:\n",
    "\n",
    "$$ P(y|\\alpha) = \\prod_i \\rho(y_i - f(x_i; \\alpha))$$\n",
    "\n",
    "Maximizing this liklihood with respect to the set of parameters $\\alpha$ will give us the parameterization of the model that best fits the given data. However, it is often more convenient (and more common) to maximize $\\log(P(y|\\alpha))$ instead.\n",
    "<br>\n",
    "\n",
    "#### <span style=\"color:red\">>>>QUESTION 4.2</span>\n",
    "\n",
    "\n",
    "Why is it okay to use the log likelihood instead of the likelihood for our likelihood function?\n",
    "\n",
    "Hint: what is the only purpose of the likelihood function we have mentioned so far? How is it affected by taking the log?\n",
    "\n",
    "<!--\n",
    "#solution\n",
    "Since all we need to do is maximize $P(y|\\alpha)$, and because the logarithm is a monotonically increasing function, maximizing $\\log( P(y|\\alpha))$ is equivalent to maximizing $ P(y|\\alpha) $\n",
    "-->\n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.2 Maximum Likelihood Estimation for Gaussian Errors\n",
    "\n",
    "Let's return to having $\\epsilon$ in $y_i = f(x_i) + \\epsilon $ be a Gaussian distribution. The likelihood distribution is just what we calculated in Question 4.1(b).\n",
    "\n",
    "We would like to find the parameters that maximize the likelihood distribution,  i.e.\n",
    "\n",
    "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( \\frac{1}{(2\\pi)^{k/2}\\prod_i\\sigma_i} \\exp\\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}\\right)\\right)$$\n",
    "\n",
    "We know that since the logarithm is a monotonically increasing function, if we maximize the logarithm of this function, we maximize the function itself. This means that\n",
    "\n",
    "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2} - \\sum_i \\log (\\sigma_i\\sqrt{2\\pi}) \\right)$$\n",
    "\n",
    "The term involving the $\\sum_i \\log (\\sigma_i\\sqrt{2\\pi})$ is a constant for the data, so we can ignore it in the maximization. \n",
    "\n",
    "\n",
    "$$\\alpha^{\\text{best}}_j = \\text{argmax}_{\\alpha_j} \\left( -\\frac{1}{2} \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}  \\right)$$\n",
    "\n",
    "We know that the $-\\frac{1}{2}$ in front just multiplies by a constant. So, if we want to maximize a function $-\\frac{1}{2}g(z)$ with respect to $z$, we can just minimize $g(z)$ with respect to $z$, which means that\n",
    "\n",
    "$$\\alpha^{\\text{best}}_j = \\text{argmin}_{\\alpha_j} \\left(\\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}  \\right)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.3 The $\\chi^2$ Function\n",
    "\n",
    "The maximum likelihood solution for the parameters can be found by minimizing the sum of squared residuals divided by the variance. This sum has a name, and it known as the $\\chi^2$. It is defined again, as\n",
    "\n",
    "$$\\chi^2(\\alpha_1, \\alpha_2 ... \\alpha_m) = \\sum_i\\frac{(y_i - f(x_i; \\alpha_1, \\alpha_2, ...))^2}{\\sigma_i^2}$$\n",
    "\n",
    "So, many fitting problems that you will encounter come down to minimizing this sum. In lecture, we had a similar sum, but without the $\\sigma_i^2$ in the denominator. If we assume that each point has the same uncertainty, than minimizing this $\\chi^2$ is the same as minimizing the sum we saw in lecture. This technique that I'm showing you is known as <b>weighted least squares</b>, as opposed to the derivation we saw in lecture that was <b>ordinary least squares</b>.\n",
    "\n",
    "In lecture, we saw how this can be solved analytically, using linear algebra or vector calculus. However, we can use the computer to numerically minimize the chi-square. The most famous of these techniques is known as gradient descent, which will be covered in one of the lectures, and is what the Python package lmfit uses. However, now we will delve deeper into the significance of chi-square and how we can use it to assess goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "Let's see what chi-squared tells us about data on a histogram. Run the code below to read data from a file and plot...\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "pred = np.genfromtxt('chisq_model.txt', delimiter=',')    # read in file\n",
    "x = np.array([n[0] for n in pred])    # get x values\n",
    "pred_y = [n[1] for n in pred]    # get y values\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'Frequency')\n",
    "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "Next, sample errors from a normal distribution and plot the data from \"chisq_data1.txt\" with Poisson error bars.\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(y):\n",
    "    plt.errorbar(x,y,yerr=np.sqrt(y),ecolor='k',elinewidth=1,capsize=4,linestyle='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.loadtxt('chisq_data1.txt', delimiter=',')    # read in file\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'Frequency')\n",
    "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n",
    "plt.scatter(x,data1,c='k')\n",
    "plot_error(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "Compute and print the Chi-squared coefficient and probability.\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2,p = chisquare(data1,pred_y)\n",
    "print(\"Chi-squared:\",chi2)\n",
    "print(\"Probability: {:.4%}\".format(p)) #a bit dodgy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "Try again for a different set of data, called \"chisq_data2.txt\". Again, compute and print the Chi-squared coefficient and probability.\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('chisq_data2.txt', delimiter=',')    # read in file\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'Frequency')\n",
    "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n",
    "plt.scatter(x,data2,c='k')\n",
    "plot_error(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2,p = chisquare(data2,pred_y)\n",
    "print(\"Chi-squared:\",chi2)\n",
    "print(\"Probability: {:.2%}\".format(p)) #quite consistent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "Try again for a different set of data, called \"chisq_data3.txt\". Again, compute and print the Chi-squared coefficient and probability.\n",
    "\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = np.loadtxt('chisq_data3.txt', delimiter=',')    # read in file\n",
    "plt.xlim(-1,1)\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'Frequency')\n",
    "plt.plot(x,pred_y,linewidth=1,drawstyle='steps')\n",
    "plt.scatter(x,data3,c='k')\n",
    "plot_error(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2,p = chisquare(data3,pred_y)\n",
    "print(\"Chi-squared:\",chi2)\n",
    "print(\"Probability: {:.2%}\".format(p)) #too consistent..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## 4.4 A Fitting Example\n",
    "\n",
    "To investigate the first step of the minimization process, let's look at a different example.\n",
    "\n",
    "Suppose you're an astrophysicist looking at a distant star. Photons hit your telescope at random, independent intervals, so the the number that you detect within your period of observation is Poisson distributed.\n",
    "\n",
    "Also, this star is really important, and $N\\gg 1$ telescopes are looking at it. Your data $D$ is therefore is $\\{n_1, n_2, \\dots, n_N\\}$ which is the counts observed by the telescopes during one day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "Let's generate some Poisson-distributed sample data for each telescope. We'll assume a parameter of $\\lambda=5$ counts per day, with $N=100$ telescopes.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import numpy as np\n",
    "\n",
    "LAMBDA = 5\n",
    "N = 100\n",
    "\n",
    "counts = np.random.poisson(LAMBDA, N);\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "LAMBDA = 5\n",
    "N = 100\n",
    "\n",
    "counts = np.random.poisson(LAMBDA, N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "Since each telescope's detection $n_i$ is independent, the probability of detecting data set $D$ given some estimate of $\\lambda$, which is the parameter, is simply the product of the probability for each telescope to detect $n_i$. This probability is Poisson distributed.\n",
    "\n",
    "We would like to use `LMFIT`'s minimize function, which does not actually request the likelihood as an input. Instead, it asks for the logarithm of the likelihood of each data point, and assumes that each data point is independent. Internally, it adds all the likelihoods from all data points to get the log-likelihood of the data.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "def log_likelihood(l, data):\n",
    "    return np.log(poisson.pmf(data, l))\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "\n",
    "def log_likelihood(l, data):\n",
    "    return np.log(poisson.pmf(data, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "\n",
    "Use `LMFIT`'s minimize function to maximize the likelihood (i.e. minimize the negative likelihood) and recover our $\\lambda=5$ value.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "from lmfit import minimize, fit_report\n",
    "\n",
    "def negative_log_likelihood(l, data):\n",
    "    return -log_likelihood(l, data)\n",
    "\n",
    "params = Parameters()\n",
    "params.add('l', min=0, value=1)\n",
    "\n",
    "result = minimize(negative_log_likelihood, params, args=(counts,))\n",
    "print(fit_report(result))\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lmfit import Parameters, minimize, fit_report\n",
    "\n",
    "def negative_log_likelihood(l, data):\n",
    "    return -log_likelihood(l, data)\n",
    "\n",
    "params = Parameters()\n",
    "params.add('l', min=0, value=1)\n",
    "\n",
    "result = minimize(negative_log_likelihood, params, args=(counts,))\n",
    "print(fit_report(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the data, true distribution, and best fit distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(np.max(counts)+1)-0.5\n",
    "xs = bins+0.5\n",
    "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Number of photons observed\")\n",
    "plt.ylabel(\"Number of telescopes\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "#### <span style=\"color:red\">>>>EXERCISE 4.1</span>\n",
    "\n",
    "To take into effect the uncertainty on $\\lambda$, compute the Poisson distribution not just for the best fit value of $\\lambda$, but also for $\\lambda - 2\\sigma_\\lambda$, $\\lambda+2\\sigma_\\lambda$, and ten $\\lambda$s in between. Store those results in a list. Here, $\\sigma_\\lambda$ is the uncertainty on $\\lambda$ generated by the fit and you can get it with `result.params['l'].stderr`.\n",
    "\n",
    "Use the code from above and add an error band (`plt.fill_between`) where the lower edge of the error band in each bin represents the lowest number of telescopes predicted among all the distributions you computed above, and the higher edge represents the highest number of telescopes.\n",
    "\n",
    "Please ask for help if you're confused about how to do this or about what you're being asked to do.\n",
    "\n",
    "<!--\n",
    "#initial code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "#your code here\n",
    "-->\n",
    "\n",
    "<!--\n",
    "#solution\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "bins = np.arange(np.max(counts)+1)-0.5\n",
    "xs = bins+0.5\n",
    "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n",
    "\n",
    "####################\n",
    "# Insert Code Here #\n",
    "####################\n",
    "\n",
    "factor = np.linspace(-2, 2, 10)\n",
    "distros = np.array([N * poisson.pmf(xs, result.params['l'].value + f * result.params['l'].stderr) for f in factor])\n",
    "minimum = [np.min(distros[:,i]) for i in range(distros.shape[1])]\n",
    "maximum = [np.max(distros[:,i]) for i in range(distros.shape[1])]\n",
    "\n",
    "####################\n",
    "\n",
    "plt.fill_between(xs, minimum, maximum, alpha=0.5, color=\"C0\")\n",
    "plt.xlabel(\"Number of photons observed\")\n",
    "plt.ylabel(\"Number of telescopes\")\n",
    "plt.legend()\n",
    "-->\n",
    "\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "bins = np.arange(np.max(counts)+1)-0.5\n",
    "xs = bins+0.5\n",
    "plt.hist(counts, bins=bins, label=\"Photon data\", fill=False, histtype=\"step\", color='k', linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs, result.params['l'].value), label=\"Best fit distro\", color=\"C0\", linewidth=2)\n",
    "plt.plot(xs, N * poisson.pmf(xs,LAMBDA), label=\"True distro\", color=\"C1\", linewidth=2)\n",
    "\n",
    "####################\n",
    "# Insert Code Here #\n",
    "####################\n",
    "\n",
    "minimum = None # Placeholder Value - Fill in the correct line\n",
    "maximum = None # Placeholder Value - Fill in the correct line\n",
    "\n",
    "####################\n",
    "\n",
    "plt.fill_between(xs, minimum, maximum, alpha=0.5, color=\"C0\")\n",
    "plt.xlabel(\"Number of photons observed\")\n",
    "plt.ylabel(\"Number of telescopes\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">>>>QUESTION 4.3</span>\n",
    "\n",
    "Suppose one of our telescopes also records information about the energy of each photon from the star. Over time, we could produce a histogram of these values representing the spectrum of the star's light output. Suppose we also have a model of the star that predicts a certain spectrum, given some parameters. How could we make a best estimate of these parameters (assuming systematic uncertainties are small relative to statistical uncertainties)?\n",
    "\n",
    "<!--\n",
    "#solution\n",
    "We could simply construct a maximum likelihood function for Poisson-distributed values about the expected spectrum and minimize with respect to the model parameters.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
